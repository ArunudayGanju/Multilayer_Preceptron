{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arunuday Ganju\n",
    "# Student id 18204644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random \n",
    "import math\n",
    "from numpy import  dot,exp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TEST 1 :Train an MLP with 2 inputs, two hidden units and one output on  XOR function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Multilayer Perceptron network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# the Network function has a parameter model_structure which depicts the network architecture.     \n",
    "def Network( model_structure):\n",
    "    \n",
    "    # the following are some global variable which would be used vy other functions in this project\n",
    "    global no_of_layers #global variable to store total number of layers in the nerural network\n",
    "    global model # global varible model has the model_structure of the network\n",
    "    global W  # global varibale W is used to store the Weights in the network\n",
    "    \n",
    "    np.random.seed(0)     \n",
    "    no_of_layers = len(model_structure) #total number of layers in the nerural network\n",
    "    \n",
    "    model = model_structure\n",
    "    W = [] # initializing the weights to a null list\n",
    "\n",
    "    # Random initialization with range of weight values (-1,1)\n",
    "    c = no_of_layers - 1\n",
    "    \n",
    "    #initializing the weights of the network between -1 and 1 by random values\n",
    "    for Layer in range(c):\n",
    "        w = 2*np.random.rand(model[Layer] + 1, model[Layer+1]) -1\n",
    "        W.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=np.exp # storing the mathematical exponent np.exp in the variable e\n",
    "\n",
    "\n",
    "# the tanh activation function which calculates tanh(x) \n",
    "def tanh(x):\n",
    "    \n",
    "    return (1.0 - e(-2*x))/(1.0 + e(-2*x))\n",
    "\n",
    "# the dy_dxTanh which calculate the derivative of the activation function for the input parameter x\n",
    "def dy_dxTanh(x):\n",
    "    \n",
    "    return (1 + tanh(x))*(1 - tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The function MultiLayerPerceptron performs the forward pass first and then the backward pass.\n",
    "def MultiLayerPerceptron( a,actual_value,alpha): #alpha is the learning rate \n",
    "    \n",
    "    # forward propagation begins \n",
    "        y = a\n",
    "        d=len(W)-1\n",
    "        for i in range(d):\n",
    "            weighted_sum = np.dot(y[i], W[i]) # the weighted sum is the sum of product of nodes and weights \n",
    "            f_of_x = tanh(weighted_sum)   # calculating the \n",
    "\n",
    "            # here the bias are being added for the subsequent layer\n",
    "            f_of_x = np.concatenate((np.ones(1), np.array(f_of_x)))\n",
    "            y.append(f_of_x)\n",
    "        \n",
    "        # end layer calculations\n",
    "        weighted_sum = np.dot(y[-1], W[-1]) #weighted_sum of the last layer \n",
    "        f_of_x = tanh(weighted_sum)    # calculating   tanh(weighted_sum) by calling the tanh() function\n",
    "        y.append(f_of_x)\n",
    "        \n",
    "    # End of forward propagation\n",
    "    \n",
    "    # begining of backward propagation\n",
    "        error = actual_value - y[-1]      # calculating the error between the acutal and predicted values \n",
    "        change = [error * dy_dxTanh(y[-1])] # calculating the change in weights as per the error computed\n",
    "\n",
    "   \n",
    "        for i in range(no_of_layers-2, 0, -1):\n",
    "            error = change[-1].dot(W[i][1:].T) \n",
    "            error = error*dy_dxTanh(y[i][1:])\n",
    "            change.append(error)\n",
    "            \n",
    "        change.reverse()\n",
    "        \n",
    "        # adjusting the weights as per the backpropagation rules\n",
    "        for i in range(len(W)):\n",
    "            Layer = y[i].reshape(1, model[i]+1)\n",
    "            delta = change[i].reshape(1, model[i+1])\n",
    "            W[i] = W[i]+ alpha*Layer.T.dot(delta)\n",
    "     # backpropagation ends here   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fit function is used to train the model based on the trainng data \n",
    "# default va;ue of epochs is taken as 100\n",
    "def fit( data, target_labels, alpha, epochs=100):\n",
    "        \n",
    "        # adding bias to input layer\n",
    "        ans = np.ones((1, data.shape[0]))\n",
    "        Zita = np.concatenate((ans.T, data), axis=1)\n",
    "        \n",
    "        for m in range(epochs):\n",
    "        \n",
    "            example = np.random.randint(X.shape[0])\n",
    "\n",
    "           \n",
    "            x = [Zita[example]]  #feed-forward propagation\n",
    "            actual_value = target_labels[example]\n",
    "            y = MultiLayerPerceptron(x,actual_value,alpha)# back-propagation of the error for weight adjsutment:\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1( x):   # function to predict one target value at a time\n",
    "    val = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "    for i in range(0, len(W)):\n",
    "        val = tanh(np.dot(val, W[i]))\n",
    "        val = np.concatenate((np.ones(1).T, np.array(val)))\n",
    "    return val[1]\n",
    "    \n",
    "    \n",
    "def predict( X):\n",
    "    value = np.array([]).reshape(0, model[-1])\n",
    "    for p in X:  # calling the predict_1 funcition in a loop to \n",
    "                 #predict values of all feature-target pair\n",
    "        y_pred = np.array([[predict_1(p)]]) \n",
    "        value = np.vstack((value,y_pred))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the network with three layers\n",
    "# input layer has 2 nodes,\n",
    "#hidden layer has 2 nodes \n",
    "#and output layer has 1 node \n",
    "Network([2,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X has the input variable for Xor operation\n",
    "X = np.array([[0, 0], [0, 1],\n",
    "                [1, 0], [1, 1]])\n",
    "\n",
    "# y has the corresponding output for given input pair\n",
    "y = np.array([0, 1, \n",
    "                 1, 0])\n",
    "\n",
    "# Call the fit function and train the network for a chosen number of epochs\n",
    "learning_rate=0.1  \n",
    "fit(X, y,learning_rate, epochs=6000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model has been successfully trained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TEST 2: At the end of training, check if the MLP predicts correctly all the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction\n",
      "[0 0]   0   [ 0.005625211435814411 ]\n",
      "[0 1]   1   [ 0.9922099656276941 ]\n",
      "[1 0]   1   [ 0.9915443580479176 ]\n",
      "[1 1]   0   [ 0.01694332658239157 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final prediction\")\n",
    "l=[]\n",
    "i=0\n",
    "for s in X:\n",
    "    p = predict_1(s)\n",
    "    if p >0.5:\n",
    "        l.append(1)\n",
    "    else:\n",
    "        l.append(0)\n",
    "    print(s,\" \",l[i],\" \",\"[\",p,\"]\")\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can observe that all the examples are correctly predicted by the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 3\n",
    "# Generate 200 vectors containing 4 components each. The value of each component should be a random number between -1 and 1. These will be your input vectors. The corresponding output for each vector should be the sin() of a combination of the components. Specifically, for inputs: [x1 x2 x3 x4] the (single component) output should be: sin(x1-x2+x3-x4) Now train an MLP with 4 inputs, at least 5 hidden units and one output on 150 of these examples and keep the remaining 50 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.41642401, -0.36855371, -0.46052122, -0.30541127, -0.54676692,\n",
       "        0.16313818, -0.61366002,  0.72602089, -0.38595787,  0.19589928,\n",
       "        0.46568234, -0.0894991 ,  0.1947524 ,  0.2871008 , -0.03543994,\n",
       "        0.55938626,  0.54254074,  0.92850961, -0.26047884,  0.04786775,\n",
       "       -0.28123204,  0.80480368,  0.49172145, -0.17084621, -0.07598683,\n",
       "        0.45755873, -0.03887989,  0.02668327, -0.15125535, -0.35819662,\n",
       "        0.30383538,  0.35487536,  0.89330487,  0.84338703, -0.38707988,\n",
       "       -0.73035558,  0.89815003,  0.05263509,  0.58442389,  0.4149357 ,\n",
       "        0.41483515, -0.24251745, -0.81860272, -0.70350402, -0.78192228,\n",
       "       -0.08685565,  0.91902416,  0.20749154,  0.0146711 ,  0.22678839])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating (200,4) random values between -1 and 1 \n",
    "\n",
    "data= np.random.rand(200,4)\n",
    "#data=np.random.randrange(-1, 1)\n",
    "X_train= data[:150]  # data kept for training \n",
    "X_test=data[150:200]  # data kept for testing , 50 rows\n",
    "Y= np.sin(data[:,0]-data[:,1]+data[:,2]-data[:,3])  #sin(x1-x2+x3-x4)\n",
    "Y_train = Y[:150]   \n",
    "Y_test=Y[150:200]\n",
    "\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 4: What is the error on training at the end? How does it compare with the error on the test set? Do you think you have learnedsatisfactorily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean sqaured error for the test data is = 0.006201271423428081\n",
      "The mean sqaured error for the train data is = 0.0069044121052475775\n",
      "The r2 score of our model is  0.9734906384200218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the NeuralNetwork with\n",
    "# 4 input neurons, 5 hidden neurons and  1 output neuron\n",
    "Network([4,5,1])\n",
    "\n",
    "learning_rate=0.1\n",
    "\n",
    "# Call the fit function and train the network for a chosen number of epochs\n",
    "fit(X_train, Y_train,learning_rate, epochs=6000)\n",
    "Y_pred= predict(X_test)\n",
    "\n",
    "\n",
    "# calclutaing the mean squared error for test data\n",
    "print(\"The mean sqaured error for the test data is =\",mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "#for s in X:\n",
    "#    print(s, nn.predict_single_data(s))\n",
    "\n",
    "Y_pred_train = predict(X_train)\n",
    "print(\"The mean sqaured error for the train data is =\",mean_squared_error(Y_train, Y_pred_train))\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(\"The r2 score of our model is \",sklearn.metrics.r2_score(Y_test,Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the mean sqaured error for the test data is = 0.006201271423428107\n",
    "T\n",
    "# The mean sqaured error for the train data is = 0.006904412105247623\n",
    "# The r2 score of our model is  .9734906384200217\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the errors generated we observe that both the errors are very similar , indicating that the model performs almost the same during training and testing, Hence the model's predictions are good even for unseen data, indicating that this is a fairly good model. The model performs well for unseen data so we can assume that there is not much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
